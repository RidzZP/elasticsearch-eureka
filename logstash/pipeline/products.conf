# Example Logstash Pipeline for Products Index
# This pipeline syncs data from MySQL to Elasticsearch for autocomplete

input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/mysql-connector-java.jar"
jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://YOUR_DB_HOST:3306/YOUR_DATABASE?serverTimezone=UTC"
    jdbc_user => "YOUR_DB_USER"
    jdbc_password => "YOUR_DB_PASSWORD"
    
    # SQL query to fetch data
    statement => "SELECT id, name, description, category, price, created_at, updated_at FROM products WHERE updated_at > :sql_last_value ORDER BY updated_at ASC"
    
    # Use tracking column for incremental updates
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    
    # Schedule: run every 5 minutes
    schedule => "*/5 * * * *"
    
    # Store last run timestamp
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run_products"
  }
}

filter {
  # Mutate plugin to transform data
  mutate {
    # Convert id to string for Elasticsearch document ID
    convert => { "id" => "string" }
    
    # Remove null fields
    remove_field => ["@version", "@timestamp"]
  }
  
  # Add suggest field for autocomplete
  mutate {
    add_field => {
      "[suggest]" => "%{name}"
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "products"
    document_id => "%{id}"
    
    # Use update action to handle incremental updates
    action => "update"
    doc_as_upsert => true
  }
  
  # Debugging output (optional - remove in production)
  stdout {
    codec => rubydebug
  }
}
