# Logstash Pipeline for Eureka Bookhouse Products
# Syncs data from db_product_description and db_gt_product_description to Elasticsearch

input {
  # Pipeline for db_product_description
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://${DB_HOST:34.101.254.58}:3306/${DB_NAME:tb_2019}?serverTimezone=UTC&useSSL=false&allowPublicKeyRetrieval=true&connectTimeout=30000&socketTimeout=60000"
    jdbc_user => "${DB_USER:root}"
    jdbc_password => "${DB_PASS:SipLah@2024}"
    jdbc_validate_connection => true
    jdbc_validation_timeout => 10
    jdbc_pool_timeout => 10
    
    # SQL query to fetch data from db_product_description
    statement => "SELECT product_id, name, seo FROM db_product_description WHERE product_id > :sql_last_value ORDER BY product_id ASC"
    
    # Use tracking column for incremental updates
    use_column_value => true
    tracking_column => "product_id"
    tracking_column_type => "numeric"
    
    # Schedule: run every 5 minutes
    schedule => "*/5 * * * *"
    
    # Store last run timestamp
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run_product_description"
    
    # Add source identifier
    add_field => { "source_table" => "db_product_description" }
  }
  
  # Pipeline for db_gt_product_description
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://${DB_HOST:34.101.254.58}:3306/${DB_NAME:tb_2019}?serverTimezone=UTC&useSSL=false&allowPublicKeyRetrieval=true&connectTimeout=30000"
    jdbc_user => "${DB_USER:root}"
    jdbc_password => "${DB_PASS:SipLah@2024}"
    jdbc_validate_connection => true
    jdbc_validation_timeout => 10
    
    # SQL query to fetch data from db_gt_product_description
    statement => "SELECT product_id, name, seo FROM db_gt_product_description WHERE product_id > :sql_last_value ORDER BY product_id ASC"
    
    # Use tracking column for incremental updates
    use_column_value => true
    tracking_column => "product_id"
    tracking_column_type => "numeric"
    
    # Schedule: run every 5 minutes
    schedule => "*/5 * * * *"
    
    # Store last run timestamp
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run_gt_product_description"
    
    # Add source identifier
    add_field => { "source_table" => "db_gt_product_description" }
  }
}

filter {
  # Mutate plugin to transform data
  mutate {
    # Convert product_id to string for Elasticsearch document ID
    convert => { "product_id" => "string" }
    
    # Remove null fields and metadata
    remove_field => ["@version"]
  }
  
  # Add suggest field for autocomplete (using name field)
  if [name] {
    mutate {
      add_field => {
        "[suggest]" => "%{name}"
      }
    }
  }
  
  # Add autocomplete field for edge n-gram search
  if [name] {
    mutate {
      add_field => {
        "[name_autocomplete]" => "%{name}"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:http://elasticsearch:9200}"]
    index => "product-ebh"
    document_id => "%{source_table}_%{product_id}"
    
    # Use update action to handle incremental updates
    action => "update"
    doc_as_upsert => true
  }
  
  # Debugging output (optional - remove in production)
  stdout {
    codec => rubydebug
  }
}
